{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import sklearn \n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open('rotten_imdb.tar/plot.5000', 'r') as f:\n",
    "    plot_data = f.read().splitlines() \n",
    "    \n",
    "len(plot_data)\n",
    "plot_label = ['objective'] * len(plot_data)\n",
    "\n",
    "with open('rotten_imdb.tar/quote.5000') as f:\n",
    "    quote_data =f.read().splitlines() \n",
    "len(quote_data)\n",
    "quote_label = ['subjective'] * len(quote_data)\n",
    "\n",
    "data = plot_data + quote_data\n",
    "label = np.array(plot_label + quote_label)\n",
    "data = np.array([d.decode('utf-8', 'ignore') for d in data])\n",
    "\n",
    "N = data.shape[0]\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(\n",
    "    data, label, test_size=0.2, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom transformer\n",
    "# Pos tagging count \n",
    "import nltk\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "class POSCountAndObjScoreTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    TAGS =  ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ'\n",
    "             , 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS'\n",
    "             , 'NNP', 'NNPS', 'PDT', 'POS', 'PRP'\n",
    "             , 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM'\n",
    "             , 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP'\n",
    "             , 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "    \n",
    "    def __init__(self, normalize=True, pos_tags=None):\n",
    "        if pos_tags:\n",
    "            self.selected_tags = [t for t in pos_tags if t in self.TAGS]\n",
    "        else:\n",
    "            self.selected_tags = self.TAGS\n",
    "        self.selected_tags_dict = {k: i for i, k in enumerate(self.selected_tags)}\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def _to_wordnet_tag(self, nltk_tag):\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif nltk_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif nltk_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return ''\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "            X: A list of string of sentences\n",
    "        Returns\n",
    "        ----------\n",
    "            A 2-d array with ...\n",
    "        \"\"\"\n",
    "        # Ignore punctuation\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        # POS counts and objective score\n",
    "        result = np.zeros((len(X), len(self.selected_tags) + 1))\n",
    "        for i, sent in enumerate(X):\n",
    "            words = tokenizer.tokenize(sent)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            for t in tagged:\n",
    "                if t[1] in self.selected_tags_dict:\n",
    "                    result[i, self.selected_tags_dict[t[1]]] += 1\n",
    "                    wordnet_tag = self._to_wordnet_tag(t[1])\n",
    "                    senti = swn.senti_synsets(t[0], wordnet_tag)\n",
    "                    if senti:\n",
    "                        result[i, -1] += senti[0].obj_score()\n",
    "\n",
    "            if self.normalize:\n",
    "                result[i] /= len(words)\n",
    "\n",
    "        return sp.csr_matrix(result)\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.13793103,  0.        ,  0.        ,  0.03448276],\n",
       "        [ 0.03333333,  0.        ,  0.        ,  0.01666667],\n",
       "        [ 0.10344828,  0.        ,  0.        ,  0.06896552],\n",
       "        [ 0.15789474,  0.        ,  0.        ,  0.05263158],\n",
       "        [ 0.11111111,  0.03703704,  0.        ,  0.06944444],\n",
       "        [ 0.28571429,  0.        ,  0.14285714,  0.21428571],\n",
       "        [ 0.07142857,  0.        ,  0.        ,  0.01785714],\n",
       "        [ 0.13888889,  0.        ,  0.        ,  0.13888889],\n",
       "        [ 0.16666667,  0.        ,  0.        ,  0.0625    ],\n",
       "        [ 0.33333333,  0.        ,  0.        ,  0.19444444]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['today is a good day', 'happy we go, ! !!']\n",
    "s = POSCountAndObjScoreTransformer(pos_tags= ['JJ', 'JJR', 'JJS']).transform(data_train).todense()\n",
    "s[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "import string \n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, stopwords=None, punct=None, lower=True, strip=True):\n",
    "        self.lower = lower\n",
    "        self.strip = strip\n",
    "        self.stopwords = stopwords or set(sw.words('english'))\n",
    "        self.punct = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transformation(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "    \n",
    "    def transform(self, X):\n",
    "        self.counter = 0\n",
    "        return [' '.join(list(self.tokenize(sent))) for sent in X]\n",
    "    \n",
    "    def tokenize(self, sent):\n",
    "        self.counter += 1\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "            # apply preprocessing to the token\n",
    "            token = token.lower() if self.lower else token\n",
    "            token = token.strip() if self.strip else token\n",
    "            token = token.strip('_') if self.strip else token\n",
    "            token = token.strip('*') if self.strip else token\n",
    "\n",
    "            # if stopword, ignore token and continue\n",
    "            if token in self.stopwords:\n",
    "                continue\n",
    "\n",
    "            # if puncutation, ignore token and continue\n",
    "            if all(char in self.punct for char in token):\n",
    "                continue\n",
    "\n",
    "            # Lemmatize the token and yield\n",
    "            lemma = self.lemmatize(token, tag)\n",
    "            yield lemma\n",
    "    \n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "        \n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessor = NLTKPreprocessor()\n",
    "new_data_train = preprocessor.transform(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "selected_tags = ['JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS', 'RB', 'RBR', 'RBS', 'RP', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "features =  ('features', FeatureUnion([\n",
    "                ('tfidf', TfidfVectorizer(max_df=0.95, min_df=2, ngram_range=(1, 3), max_features=10000000))\n",
    "                , ('pos', POSCountAndObjScoreTransformer(pos_tags=selected_tags))])\n",
    "            )\n",
    "\n",
    "bayes_pipeline = Pipeline([\n",
    "    features\n",
    "    , ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    features\n",
    "    , ('clf', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SGDClassifier(loss='hinge', penalty='12', shuffle=True)\n",
    "\n",
    "OnveVsRestClassifier()\n",
    "\n",
    "CalibratedClassifier()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('tfidf', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.95, max_features=None, min_df=2,\n",
       "        ngr...    transformer_weights=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'features__pos__pos_tags': [['JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS', 'RB', 'RBR', 'RBS', 'RP', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'], ['JJ', 'JJR', 'JJS'], []]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bayes grid search\n",
    "\n",
    "selected_tags_param = [\n",
    "    ['JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS', 'RB', 'RBR', 'RBS', 'RP', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    , ['JJ', 'JJR', 'JJS']\n",
    "    , []]\n",
    "\n",
    "params_grid = [\n",
    "    {'features__pos__pos_tags': selected_tags_param}\n",
    "]\n",
    "\n",
    "bayes_gs = GridSearchCV(bayes_pipeline, params_grid)\n",
    "bayes_gs.fit(data_train, label_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('tfidf', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.95, max_features=None, min_df=2,\n",
       "        ngr...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'clf__C': [1], 'clf__kernel': ['linear']}, {'clf__gamma': [0.001, 0.0001], 'clf__C': [1], 'clf__kernel': ['rbf']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM grid search\n",
    "\n",
    "params_grid = [\n",
    "    {'clf__C': [1], 'clf__kernel': ['linear']},\n",
    "    {'clf__C': [1], 'clf__kernel': ['rbf'], 'clf__gamma': [0.001, 0.0001]}\n",
    "]\n",
    "\n",
    "svm_gs = GridSearchCV(svm_pipeline, params_grid)\n",
    "svm_gs.fit(data_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_clf__C</th>\n",
       "      <th>param_clf__gamma</th>\n",
       "      <th>param_clf__kernel</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.700667</td>\n",
       "      <td>10.174667</td>\n",
       "      <td>0.910625</td>\n",
       "      <td>0.991625</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear</td>\n",
       "      <td>{u'clf__C': 1, u'clf__kernel': u'linear'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.913418</td>\n",
       "      <td>0.99156</td>\n",
       "      <td>0.911853</td>\n",
       "      <td>0.991189</td>\n",
       "      <td>0.906602</td>\n",
       "      <td>0.992126</td>\n",
       "      <td>1.195918</td>\n",
       "      <td>0.931347</td>\n",
       "      <td>2.915315e-03</td>\n",
       "      <td>3.854017e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.703000</td>\n",
       "      <td>12.159000</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{u'clf__gamma': 0.001, u'clf__C': 1, u'clf__ke...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.501499</td>\n",
       "      <td>0.50150</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.991710</td>\n",
       "      <td>0.608156</td>\n",
       "      <td>5.302638e-07</td>\n",
       "      <td>2.651982e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.323333</td>\n",
       "      <td>13.213333</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{u'clf__gamma': 0.0001, u'clf__C': 1, u'clf__k...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.501499</td>\n",
       "      <td>0.50150</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.936183</td>\n",
       "      <td>0.855864</td>\n",
       "      <td>5.302638e-07</td>\n",
       "      <td>2.651982e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0      21.700667        10.174667         0.910625          0.991625   \n",
       "1      26.703000        12.159000         0.501500          0.501500   \n",
       "2      26.323333        13.213333         0.501500          0.501500   \n",
       "\n",
       "  param_clf__C param_clf__gamma param_clf__kernel  \\\n",
       "0            1              NaN            linear   \n",
       "1            1            0.001               rbf   \n",
       "2            1           0.0001               rbf   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "0          {u'clf__C': 1, u'clf__kernel': u'linear'}                1   \n",
       "1  {u'clf__gamma': 0.001, u'clf__C': 1, u'clf__ke...                2   \n",
       "2  {u'clf__gamma': 0.0001, u'clf__C': 1, u'clf__k...                2   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0           0.913418             0.99156           0.911853   \n",
       "1           0.501499             0.50150           0.501500   \n",
       "2           0.501499             0.50150           0.501500   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0            0.991189           0.906602            0.992126      1.195918   \n",
       "1            0.501500           0.501500            0.501500      0.991710   \n",
       "2            0.501500           0.501500            0.501500      0.936183   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.931347    2.915315e-03     3.854017e-04  \n",
       "1        0.608156    5.302638e-07     2.651982e-07  \n",
       "2        0.855864    5.302638e-07     2.651982e-07  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "bayes_results = pd.DataFrame(bayes_gs.cv_results_)\n",
    "bayes_results\n",
    "\n",
    "svm_results = pd.DataFrame(svm_gs.cv_results_)\n",
    "svm_results\n",
    "\n",
    "# print(\"Detailed classification report:\")\n",
    "# print()\n",
    "# print(\"The model is trained on the full development set.\")\n",
    "# print(\"The scores are computed on the full evaluation set.\")\n",
    "# print()\n",
    "# y_true, y_pred = y_test, bayes_gs.predict(X_test)\n",
    "# print(classification_report(y_true, y_pred))\n",
    "# print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
